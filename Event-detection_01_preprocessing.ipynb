{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# setup for logging\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# write logs with time to log folder\n",
    "LOG_FILENAME = datetime.now().strftime('~/log/logfile_%H_%M_%S_%d_%m_%Y.log')\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(filename=LOG_FILENAME,level=logging.DEBUG)\n",
    "\n",
    "# get data\n",
    "with open('data.json', 'r') as f:\n",
    "    datastore = json.load(f)\n",
    "\n",
    "# make dataframe\n",
    "df = pd.DataFrame(datastore)\n",
    "\n",
    "logging.info('Dataframe created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count duplicate rows\n",
    "count = df.id.duplicated().sum()\n",
    "print(\"Number of duplicates before removal:\", count)\n",
    "\n",
    "# drop dups based on id\n",
    "df_sample_sorted = df.sort_values('id', ascending=True)\n",
    "df_sample_unique = df_sample_sorted.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "# count dups again\n",
    "count_expost = df_sample_unique.id.duplicated().sum()\n",
    "print(\"Number of duplicates after removal:\", count_expost)\n",
    "\n",
    "logging.info(\"Dups removed\")\n",
    "\n",
    "# replace string timestamps with datetime\n",
    "df_sample_unique['timestamp'] = df_sample_unique['timestamp'].astype('datetime64[ns]')\n",
    "\n",
    "# sort by timestamp values\n",
    "df_sample_bytime = df_sample_unique.sort_values('timestamp', ascending=True)\n",
    "\n",
    "# reset index and drop old to avoid making new column with old index\n",
    "df_sample_bytime = df_sample_bytime.reset_index(drop=True)\n",
    "\n",
    "# add column with index values to use as id\n",
    "df_sample_bytime_andindex = df_sample_bytime.reset_index(drop=False)\n",
    "\n",
    "# drop columns _id, id\n",
    "df_sample_final = df_sample_bytime_andindex.drop(['_id', 'id'], axis=1)\n",
    "\n",
    "logging.info(\"Basic cleaning completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "import preprocessor as p\n",
    "\n",
    "# removes hashtags, mentions, emoji and URLs\n",
    "df_sample_final['text'] = df_sample_final['text'].apply(p.clean)\n",
    "\n",
    "logging.info(\"Preprocessor worked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from langdetect import detect\n",
    "\n",
    "# define function\n",
    "def try_detect(cell):\n",
    "    try:\n",
    "        detected_lang = detect(cell)\n",
    "    except:\n",
    "        detected_lang = None\n",
    "    return detected_lang\n",
    "\n",
    "# apply function and store result in lang column\n",
    "df_sample_final['lang'] = df_sample_final.text.apply(try_detect)\n",
    "\n",
    "logging.info(\"Langdetect complete\")\n",
    "\n",
    "# keep only Turkish (includes dropna!)\n",
    "target = ['tr']\n",
    "df_sample_final.drop(df_sample_final[df_sample_final.lang.isin(target) == False].index , inplace=True)\n",
    "\n",
    "logging.info(\"Other languages removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure separation of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remover function\n",
    "def remover(stringput):\n",
    "    dic={\"'\":\" \",\"-\":\" \", \",\":\" \", \"/\":\" \",\"_\":\" \",\"&\":\" \",\"*\":\" \",\":\":\" \",\"+\":\" \",\".\":\" \", \"â€™\":\" \", \"!\":\" \", \"?\":\" \", \"(\":\" \", \")\":\" \", \";\":\" \"}\n",
    "    output=\"\".join((dic.get(x,x) for x in stringput))\n",
    "    return output\n",
    "\n",
    "# replaces apostrophs and joining elements with space to ensure separation of words\n",
    "df_sample_final['text'] = df_sample_final['text'].apply(remover)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import NER toolkit\n",
    "import polyglot\n",
    "from polyglot.text import Text, Word\n",
    "import subprocess\n",
    "\n",
    "subprocess.run('polyglot download embeddings2.tr ner2.tr', shell=True)\n",
    "\n",
    "%%bash\n",
    "polyglot download embeddings2.tr ner2.tr\n",
    "\n",
    "# define extractor function\n",
    "def ner(cell):\n",
    "    discover = Text(cell, hint_language_code='tr')\n",
    "    return discover.entities\n",
    "\n",
    "# apply function to the tweets and store output in labeled_ner\n",
    "df_sample_final['labeled_ner'] = df_sample_final['text'].apply(ner)\n",
    "\n",
    "logging.info(\"NER success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stopwords from .txt file\n",
    "with open('STOPFILE.txt', 'r') as f:\n",
    "    stopwords = []\n",
    "    for line in f:\n",
    "        stopwords.append(line.strip().split(','))\n",
    "        \n",
    "f.close()\n",
    "\n",
    "# make list\n",
    "stopwords = [i[0] for i in stopwords]\n",
    "\n",
    "# remove dups in list to ensure clean output\n",
    "stopwords = list(dict.fromkeys(stopwords))\n",
    "\n",
    "# define function\n",
    "def stopcleaner(cell):\n",
    "    dic = {}\n",
    "    tokens = cell.lower().split()\n",
    "    for word in tokens:\n",
    "        if word in stopwords:\n",
    "            z = {word:\"\"}\n",
    "            dic.update(z)\n",
    "    output = \" \".join(dic.get(x,x) for x in tokens)\n",
    "    return output\n",
    "\n",
    "# tokenizes text feature and removes stop word noise\n",
    "df_sample_final['text'] = df_sample_final['text'].apply(stopcleaner)\n",
    "\n",
    "logging.info(\"Stopwords are out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize list\n",
    "places = []\n",
    "\n",
    "# get file\n",
    "infile = open('Neighborhoods.txt', 'r')\n",
    "for line in infile:\n",
    "    places.append(line.strip().split(','))\n",
    "\n",
    "# close file\n",
    "infile.close()\n",
    "\n",
    "# get next\n",
    "infile = open('POI.txt', 'r')\n",
    "for line in infile:\n",
    "    places.append(line.strip().split(','))\n",
    "\n",
    "# close file\n",
    "infile.close()\n",
    "\n",
    "# get next\n",
    "infile = open('Bridges.txt', 'r')\n",
    "for line in infile:\n",
    "    places.append(line.strip().split(','))\n",
    "\n",
    "# close file\n",
    "infile.close()\n",
    "\n",
    "# get next\n",
    "infile = open('Stations.txt', 'r')\n",
    "for line in infile:\n",
    "    places.append(line.strip().split(','))\n",
    "\n",
    "# close file\n",
    "infile.close()\n",
    "\n",
    "# get next\n",
    "infile = open('Mosques.txt', 'r')\n",
    "for line in infile:\n",
    "    places.append(line.strip().split(','))\n",
    "\n",
    "# close file\n",
    "infile.close()\n",
    "\n",
    "# make proper list (not list of lists)\n",
    "places = [i[0] for i in places]\n",
    "\n",
    "# remove dups in list to ensure clean output\n",
    "places = list(dict.fromkeys(places))\n",
    "\n",
    "# getlocs function\n",
    "def getlocs(cell):\n",
    "    out = []\n",
    "    for item in places:\n",
    "        x = str(item)\n",
    "        if x in cell or x.lower() in cell:\n",
    "            out.append(item.lower())\n",
    "    return out\n",
    "\n",
    "# collects all location mentions\n",
    "df_sample_final['ist_locations'] = df_sample_final['text'].apply(getlocs)\n",
    "\n",
    "logging.info(\"Location mentions done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting and lemmatizing informative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from cube.api import Cube\n",
    "\n",
    "#initialize\n",
    "cube=Cube(verbose=True)\n",
    "\n",
    "#load model\n",
    "cube.load(\"tr\")\n",
    "\n",
    "def getall(cell):\n",
    "    output = []\n",
    "    sentences=cube(cell)\n",
    "    target = [\"VERB\", \"NOUN\", \"ADJ\", \"PRON\"]\n",
    "    for sentence in sentences:\n",
    "        for entry in sentence:\n",
    "            if entry.upos in target:\n",
    "                output.append(str(entry.lemma))\n",
    "    return output\n",
    "\n",
    "# import\n",
    "from multiprocess import Pool\n",
    "import time\n",
    "\n",
    "# initialize to 96 cores\n",
    "pool = Pool(96)\n",
    "\n",
    "# define series\n",
    "df_tho = df_sample_final[\"text\"]\n",
    "\n",
    "# start timer\n",
    "t0 = time.time()\n",
    "\n",
    "# execute a computation in parallel\n",
    "result = pool.map(getall, df_tho)\n",
    "\n",
    "# turn off workers\n",
    "pool.close()\n",
    "\n",
    "# record the time\n",
    "t1 = time.time()\n",
    "print(\"Time: {}\".format(t1-t0))\n",
    "\n",
    "# append result to dataframe\n",
    "df_sample_final[\"content\"] = result\n",
    "\n",
    "logging.info(\"Informative words extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transliteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transliteration function\n",
    "\n",
    "def transliterate(item):\n",
    "    dic={\"Ä±\":\"i\",\"ÄŸ\":\"g\",\"ÅŸ\":\"s\",\"Ã§\":\"c\",\"Ã¼\":\"u\",\"Ã¶\":\"o\", \"Ä°\":\"i\", \"Ã–\":\"Ã¶\", \"Ã‡\":\"Ã§\", \"Ãœ\":\"Ã¼\", \"I\":\"i\",\"Åž\":\"s\"}\n",
    "    output=\"\".join((dic.get(x,x) for x in item))\n",
    "    return output\n",
    "\n",
    "# function that applies transliterate to a list\n",
    "def transliminator(listput):\n",
    "    output = []\n",
    "    for item in listput:\n",
    "        result = transliterate(item)\n",
    "        output.append(result)\n",
    "    return output\n",
    "\n",
    "# apply function\n",
    "df_sample_final['content_norm'] = df_sample_final['content'].apply(transliminator)\n",
    "\n",
    "logging.info(\"Tweets transliminated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine outputs and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list from list of list\n",
    "def flattener(l):\n",
    "    flat_list = [item for sublist in l for item in sublist]\n",
    "    return flat_list\n",
    "\n",
    "# flatten, transliminate, combine\n",
    "df_sample_final[\"ner_flat\"] = df_sample_final[\"labeled_ner\"].apply(flattener)\n",
    "df_sample_final[\"ner_flat\"] = df_sample_final[\"ner_flat\"].apply(transliminator)\n",
    "df_sample_final[\"combined\"] = df_sample_final[\"ner_flat\"] + df_sample_final[\"ist_locations\"] + df_sample_final[\"content_norm\"]\n",
    "\n",
    "# make final one .lower() and ensure each word only appears once\n",
    "\n",
    "# function that makes each item in list .lower()\n",
    "def lowerit(listput):\n",
    "    output = []\n",
    "    for item in listput:\n",
    "        result = str(item.lower())\n",
    "        output.append(result)\n",
    "    return output\n",
    "\n",
    "# remove dups in list to ensure clean output\n",
    "def removedups(stuff):\n",
    "    new = list(dict.fromkeys(stuff))\n",
    "    return new\n",
    "\n",
    "df_sample_final[\"combined\"] = df_sample_final[\"combined\"].apply(lowerit)\n",
    "df_sample_final[\"combined\"] = df_sample_final[\"combined\"].apply(transliminator)\n",
    "df_sample_final[\"combined\"] = df_sample_final[\"combined\"].apply(removedups)\n",
    "\n",
    "# file export\n",
    "\n",
    "with open('df_preprocessed.pkl', 'wb') as f:\n",
    "    pickle.dump(df_sample_final, f)\n",
    "\n",
    "f.close()\n",
    "\n",
    "logging.info(\"File is out\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
