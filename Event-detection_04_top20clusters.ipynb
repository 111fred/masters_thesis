{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# setup for logging\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# write logs with time to log folder\n",
    "LOG_FILENAME = datetime.now().strftime('/home/wgrambozambo/log/logfile_%H_%M_%S_%d_%m_%Y.log')\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(filename=LOG_FILENAME,level=logging.DEBUG)\n",
    "\n",
    "# open file\n",
    "with open('data_clustered.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "f.close()\n",
    "\n",
    "# sort by timestamp\n",
    "df = df.sort_values('ts', ascending=True)\n",
    "\n",
    "logging.info('Dataframe ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into time slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that splits df by time frames\n",
    "\n",
    "def splitter(frame):\n",
    "    values = [i for i in range(0, df['slot'].max()+1)]\n",
    "    frames = []\n",
    "    for a in values:\n",
    "        df1 = frame.loc[frame['slot'] == a]\n",
    "        frames.append(df1)\n",
    "    return frames\n",
    "\n",
    "# split into frames\n",
    "frames = splitter(df)\n",
    "\n",
    "logging.info('Dataframe split')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize\n",
    "This needs to be redone because of the clusterids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.feature_extraction\n",
    "\n",
    "def vectorboy_arrays(lizt):\n",
    "    rays = []\n",
    "    for item in lizt:\n",
    "        num = (max(int(item.shape[0]*0.0025),8))\n",
    "        vectorizer = sklearn.feature_extraction.text.CountVectorizer(min_df=num)\n",
    "        corpus = item.documents.tolist()\n",
    "        X = vectorizer.fit_transform(corpus).toarray()\n",
    "        rays.append(X)\n",
    "    return rays\n",
    "\n",
    "def vectorboy_vocabs(lizt):\n",
    "    vocabs = []\n",
    "    for item in lizt:\n",
    "        num = (max(int(item.shape[0]*0.0025),8))\n",
    "        vectorizer = sklearn.feature_extraction.text.CountVectorizer(min_df=num)\n",
    "        corpus = item.documents.tolist()\n",
    "        X = vectorizer.fit_transform(corpus).toarray()\n",
    "        vocabs.append(vectorizer.get_feature_names())\n",
    "    return vocabs\n",
    "\n",
    "# generate slot vector matrices list\n",
    "rays = vectorboy_arrays(frames)\n",
    "\n",
    "# generate corresponding vocabs list\n",
    "vocabs = vectorboy_vocabs(frames)\n",
    "\n",
    "logging.info('Arrays and vocabs done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute dfidft weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df idft\n",
    "from collections import OrderedDict\n",
    "import time \n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def tf(voc, arr):\n",
    "    output = []\n",
    "    for f, b in zip(voc, arr):\n",
    "        result = np.sum(b, axis = 0)\n",
    "        result = result.tolist()\n",
    "        dikt = OrderedDict(zip(f, result))\n",
    "        output.append(dikt)\n",
    "    return output\n",
    "\n",
    "# start timer\n",
    "t0 = time.time()\n",
    "\n",
    "# simple term frequencies\n",
    "tf = tf(vocabs, rays)\n",
    "\n",
    "# previous df average per word in vocab of time slot\n",
    "def tf_pre(lizt):\n",
    "    output = []\n",
    "    for idx, val in enumerate(lizt):\n",
    "        diktator = {}\n",
    "        if idx < 4:\n",
    "            for item in val:\n",
    "                diktator.update({item : 0})\n",
    "        else:\n",
    "            dikt_1 = lizt[(idx-1)]\n",
    "            dikt_2 = lizt[(idx-2)]\n",
    "            dikt_3 = lizt[(idx-3)]\n",
    "            dikt_4 = lizt[(idx-4)]\n",
    "            for item in val:\n",
    "                n1 = dikt_1.get(item, 0)\n",
    "                n2 = dikt_2.get(item, 0)\n",
    "                n3 = dikt_3.get(item, 0)\n",
    "                n4 = dikt_4.get(item, 0)\n",
    "                average = (n1+n2+n3+n4)/4\n",
    "                diktator.update({item : average})\n",
    "        output.append(diktator)\n",
    "    return output\n",
    "\n",
    "# term frequencies in previous slots\n",
    "previous = tf_pre(tf)\n",
    "\n",
    "def dfidft(now, prev):\n",
    "    output = []\n",
    "    for f, b in zip(now, prev):\n",
    "        diktator = {}\n",
    "        for item in f:\n",
    "            tf = f.get(item)\n",
    "            ptf = b.get(item)\n",
    "            weight = (tf+1)/(math.log(ptf+1)+1)\n",
    "            diktator.update({item : weight})\n",
    "        output.append(diktator)\n",
    "    return output\n",
    "                             \n",
    "# dfidft                         \n",
    "weighted_vocabs = dfidft(tf, previous)\n",
    "                             \n",
    "# record the time\n",
    "t1 = time.time()\n",
    "print(\"Time: {}\".format(t1-t0))\n",
    "\n",
    "logging.info('Dfidft complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase weights for named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make ner list\n",
    "ner_list = df['ner_all'].tolist()\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "# ner list with unique values \n",
    "ner_list = list(set(chain(*ner_list)))\n",
    "\n",
    "# with ner list\n",
    "\n",
    "def nert(lizt):\n",
    "    output = []\n",
    "    for a in lizt:\n",
    "        diktator = {}\n",
    "        for item in a:\n",
    "            v = a.get(item)\n",
    "            if item in ner_list:\n",
    "                v = v * 2.5\n",
    "            else:\n",
    "                v = v\n",
    "            diktator.update({item : v})\n",
    "        output.append(diktator)\n",
    "    return output\n",
    "\n",
    "# weighted slot vocabularies final\n",
    "wvoc_final = nert(weighted_vocabs)\n",
    "\n",
    "logging.info('Weighted vocabs updated with NER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def clusterman(lizt):\n",
    "    output = []\n",
    "    for item in lizt:\n",
    "        serie = item['clusterid']\n",
    "        clusterray = np.array(serie.values.tolist())\n",
    "        base = np.unique(clusterray, return_counts=True)\n",
    "        keyz = base[0].tolist()\n",
    "        valuez = base[1].tolist()\n",
    "        base = dict(zip(keyz, valuez))\n",
    "        target = []\n",
    "        for a in base:\n",
    "            b = base.get(a)\n",
    "            if b >= 10:\n",
    "                target.append(a)\n",
    "        for c in target:\n",
    "            yo = [c]\n",
    "            df = item[item.clusterid.isin(yo)]\n",
    "            # drop clusterid col\n",
    "            # df.drop(['clusterid'], axis=1)\n",
    "            # add uuid\n",
    "            # df['cid'] = uuid.uuid4()\n",
    "            output.append(df)\n",
    "    return output\n",
    "\n",
    "# list of clusters\n",
    "clusterlist = clusterman(frames)\n",
    "\n",
    "logging.info('List of clusters generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get weighted cluster vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# with clusterlist as lizt1 and weighted_vocabs as lizt2\n",
    "def dictgetter(lizt1, lizt2):\n",
    "    output = []\n",
    "    for item in lizt1:\n",
    "        key = item.slot[0] # get slot id\n",
    "        vocab = lizt2[key] # get corresponding vocab\n",
    "        wordlist = item['combined'].tolist() # get all words used in the cluster\n",
    "        wordlist = list(set(chain(*wordlist))) # make list values unique\n",
    "        diktator = {} \n",
    "        for word in vocab:\n",
    "            val = vocab.get(word)\n",
    "            if word in wordlist:\n",
    "                diktator.update({word:val})\n",
    "        output.append(diktator)\n",
    "    return output    \n",
    "\n",
    "# weighted cluster vocabs\n",
    "clustervocabs = dictgetter(clusterlist, wvoc_final)\n",
    "\n",
    "logging.info('Weighted cluster vocabs success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average term scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def ranker_ats(a,b):\n",
    "    output = []\n",
    "    for idx, val in enumerate(a):\n",
    "        weightedvocab = b[idx]\n",
    "        weights = list(weightedvocab.values())\n",
    "        if len(weights) == 0:\n",
    "            print(\"blop\")\n",
    "            dic = {idx : 0}\n",
    "            output.append(dic)\n",
    "        else:\n",
    "            result = statistics.mean(weights)\n",
    "            dic = {idx : result}\n",
    "            output.append(dic)\n",
    "    return output\n",
    "\n",
    "# produce dict of cluster index and average term score     \n",
    "cluster_ats = ranker_ats(clusterlist, clustervocabs) \n",
    "\n",
    "logging.info('Average term scores success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get top 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that makes a list of lists: [[clusterlist[i], slotid, cluster_ats value], ... ...]\n",
    "def lolmaker_clusterrank(a,b):\n",
    "    output = []\n",
    "    for idx, val in enumerate(a):\n",
    "        bdict = b[idx]\n",
    "        bval = bdict.get(idx)\n",
    "        data = val\n",
    "        slot = data.iloc[0,6]\n",
    "        lizt = [data, slot, bval]\n",
    "        output.append(lizt)\n",
    "    return output\n",
    "        \n",
    "clusterframes = lolmaker_clusterrank(clusterlist, cluster_ats) \n",
    "\n",
    "# for clusterframes, add the weighted cluster dictionary to cluster - just for final results\n",
    "def voc_add(a,b):\n",
    "    output = []\n",
    "    for idx, val in enumerate(a):\n",
    "        bdict = b[idx]\n",
    "        res = val\n",
    "        res.append(bdict)\n",
    "        output.append(res)\n",
    "    return output\n",
    "\n",
    "clusterframes2 = voc_add(clusterframes, clustervocabs)\n",
    "\n",
    "from operator import itemgetter \n",
    "\n",
    "def top20(a):\n",
    "    output = []\n",
    "    for i in list(range(312)):\n",
    "        candidates = []\n",
    "        for item in a:\n",
    "            if item[1] == i:\n",
    "                candidates.append(item)\n",
    "        res = sorted(candidates, key = itemgetter(2), reverse=True)\n",
    "        res = res[0:20]\n",
    "        output = output + res\n",
    "    return output\n",
    "\n",
    "# this is a list of lists: the top 20 ranked by slot, including: df, slotid, average weight, term dictionary.\n",
    "ranked_by_slot = top20(clusterframes2)\n",
    "\n",
    "logging.info('Top 20 extracted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract headlines with weight score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ranked_by_slot to make a dataframe of all the first tweets of the clusters\n",
    "\n",
    "def rower(a):\n",
    "    out = pd.DataFrame(columns=['idx', 'text', 'ts', 'combined', 'ner_all', 'documents', 'slot'])\n",
    "    for idx, val in enumerate(a):\n",
    "        df = val[0]\n",
    "        row = df.iloc[0]\n",
    "        out.loc[idx] = row\n",
    "    return out\n",
    "\n",
    "# the df to perform hierarchical clustering on\n",
    "headlines = rower(ranked_by_slot)\n",
    "\n",
    "def scorer(a, b):\n",
    "    combined = a['combined'].tolist()\n",
    "    slots = a['slot'].tolist()\n",
    "    scores = []\n",
    "    for c,d in zip(combined,slots):\n",
    "        score = 0\n",
    "        y = b[d]\n",
    "        for word in c:\n",
    "            if word in y:\n",
    "                value = y.get(word)\n",
    "                score = score+value\n",
    "        scores.append(score)\n",
    "            \n",
    "    ser = pd.Series(scores)\n",
    "    df = a\n",
    "    df['weight'] = ser.values\n",
    "    return df\n",
    "\n",
    "# add the scores\n",
    "headlines_wscore = scorer(headlines, wvoc_final)\n",
    "\n",
    "logging.info('Scores added to cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide by days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that splits df by time frames\n",
    "\n",
    "def splitter(frame):\n",
    "    values = [i for i in range(0, frame['day'].max()+1)]\n",
    "    frames = []\n",
    "    for a in values:\n",
    "        df1 = frame.loc[frame['day'] == a]\n",
    "        frames.append(df1)\n",
    "    return frames\n",
    "\n",
    "# split\n",
    "hl_days = splitter(headlines_wscore)\n",
    "\n",
    "logging.info('Day split done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.feature_extraction\n",
    "\n",
    "def vectorboy_arrays(lizt):\n",
    "    rays = []\n",
    "    for item in lizt:\n",
    "        num = (max(int(item.shape[0]*0.0025),8))\n",
    "        vectorizer = sklearn.feature_extraction.text.CountVectorizer(min_df=num)\n",
    "        corpus = item.documents.tolist()\n",
    "        X = vectorizer.fit_transform(corpus).toarray()\n",
    "        rays.append(X)\n",
    "    return rays\n",
    "\n",
    "def vectorboy_vocabs(lizt):\n",
    "    vocabs = []\n",
    "    for item in lizt:\n",
    "        num = (max(int(item.shape[0]*0.0025),8))\n",
    "        vectorizer = sklearn.feature_extraction.text.CountVectorizer(min_df=num)\n",
    "        corpus = item.documents.tolist()\n",
    "        X = vectorizer.fit_transform(corpus).toarray()\n",
    "        vocabs.append(vectorizer.get_feature_names())\n",
    "    return vocabs\n",
    "\n",
    "frames = hl_days\n",
    "\n",
    "# generate vector matrices\n",
    "rays = vectorboy_arrays(frames)\n",
    "\n",
    "# generate vocabs\n",
    "vocabs = vectorboy_vocabs(frames)\n",
    "\n",
    "logging.info('Vectorization complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling and normalizing the vector matrices\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "def scaler(lizt):\n",
    "    output = []\n",
    "    for item in lizt:\n",
    "        scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "        X_scaled = scaler.fit_transform(item)\n",
    "        X_scaled = X_scaled.astype('float16')\n",
    "        output.append(X_scaled)\n",
    "    return output\n",
    "\n",
    "def normalizer(lizt):\n",
    "    output = []\n",
    "    for item in lizt:\n",
    "        X_norm = preprocessing.normalize(item)\n",
    "        X_norm = X_norm.astype('float16')\n",
    "        output.append(X_norm)\n",
    "    return output\n",
    "\n",
    "# scaling\n",
    "rays_scaled = scaler(rays)\n",
    "\n",
    "# normalizing\n",
    "rays_sn = normalizer(rays_scaled)\n",
    "\n",
    "logging.info('Normalization complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now pairwise similarity by cosine distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def cosine_sim(lizt):\n",
    "    output = []\n",
    "    for array in lizt:\n",
    "        array1 = array.astype('float16')\n",
    "        X_sim = 1-sklearn.metrics.pairwise.cosine_similarity(array1)\n",
    "        array2 = X_sim.astype('float16')\n",
    "        output.append(array2)\n",
    "    return output\n",
    "\n",
    "import time \n",
    "\n",
    "# start timer\n",
    "t0 = time.time()\n",
    "\n",
    "# apply function\n",
    "rays_cs = cosine_sim(rays)\n",
    "\n",
    "# record the time\n",
    "t1 = time.time()\n",
    "print(\"Time: {}\".format(t1-t0))\n",
    "\n",
    "logging.info('Cosine similarity success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchical clustering for looking at data\n",
    "import fastcluster\n",
    "from scipy.cluster.hierarchy import cut_tree\n",
    "\n",
    "def clusterer(lizt):\n",
    "    output = []\n",
    "    for item in lizt:\n",
    "        # load array by time slot\n",
    "        array = item\n",
    "\n",
    "        # cluster\n",
    "        X_clustered = fastcluster.linkage(array, method='centroid', metric='cosine')\n",
    "\n",
    "        cutoff = 3.5/len(array)\n",
    "        cutoff = (round(cutoff,5))\n",
    "\n",
    "        # cut tree\n",
    "        cutree = cut_tree(X_clustered, height=cutoff)\n",
    "\n",
    "        # add to output list\n",
    "        output.append(cutree)\n",
    "    return output\n",
    "\n",
    "# start timer\n",
    "t0 = time.time()\n",
    "\n",
    "cutrees = clusterer(rays_cs)\n",
    "\n",
    "# record the time\n",
    "t1 = time.time()\n",
    "print(\"Time: {}\".format(t1-t0))\n",
    "\n",
    "logging.info('Clustering success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustertagger(lizt1, lizt2):\n",
    "    output = []\n",
    "    for f, b in zip(lizt1, lizt2):\n",
    "        l1 = b.tolist()\n",
    "        my_list = [item for sublist in l1 for item in sublist]\n",
    "        f['clusterid'] = pd.Series(my_list).values\n",
    "        output.append(f)\n",
    "    return output\n",
    "\n",
    "# dataframe tagged with clusterids\n",
    "daylevel_clustered = clustertagger(frames, cutrees)\n",
    "\n",
    "logging.info('Cluster tagging success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterman(lizt):\n",
    "    output = []\n",
    "    for item in lizt:\n",
    "        serie = item['clusterid']\n",
    "        clusterray = np.array(serie.values.tolist())\n",
    "        base = np.unique(clusterray, return_counts=True)\n",
    "        keyz = base[0].tolist()\n",
    "        valuez = base[1].tolist()\n",
    "        base = dict(zip(keyz, valuez))\n",
    "        target = []\n",
    "        for a in base:\n",
    "            b = base.get(a)\n",
    "            if b >= 1:\n",
    "                target.append(a)\n",
    "        for c in target:\n",
    "            yo = [c]\n",
    "            df = item[item.clusterid.isin(yo)]\n",
    "            # drop clusterid col\n",
    "            # df.drop(['clusterid'], axis=1)\n",
    "            # add uuid\n",
    "            # df['cid'] = uuid.uuid4()\n",
    "            output.append(df)\n",
    "    return output\n",
    "\n",
    "# list of day level clusters \n",
    "dayclusters = clusterman(daylevel_clustered)\n",
    "\n",
    "def vectorboy_vocabs(lizt):\n",
    "    vocabs = []\n",
    "    for item in lizt:\n",
    "        num = (max(int(item.shape[0]*0.0025),8))\n",
    "        vectorizer = sklearn.feature_extraction.text.CountVectorizer(min_df=1)\n",
    "        corpus = item.documents.tolist()\n",
    "        X = vectorizer.fit_transform(corpus).toarray()\n",
    "        vocabs.append(vectorizer.get_feature_names())\n",
    "    return vocabs\n",
    "\n",
    "# corresponding vocabs\n",
    "clustervocs = vectorboy_vocabs(dayclusters)\n",
    "\n",
    "logging.info('Clusters prepared')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract earliest and top ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each cluster, keep only the top ranked one and the earliest. Add column that specifies r t\n",
    "def keeper(x):\n",
    "    out = pd.DataFrame(columns=['idx', 'text', 'ts', 'combined', 'ner_all', 'documents', 'slot', 'weight', 'day', 'clusterid', 'cdex', 'type'])\n",
    "    for i, item in enumerate(x):\n",
    "        item['cdex'] = i\n",
    "        rank = item['weight'].max()\n",
    "        early = item['ts'].min()\n",
    "        ranked = item[item['weight']==rank]\n",
    "        ranked['type'] = \"r\"\n",
    "        earliest = item[item['ts']==early]\n",
    "        earliest['type'] = \"e\"\n",
    "        out = out.append(earliest, ignore_index=True)\n",
    "        out = out.append(ranked, ignore_index=True)\n",
    "    return out\n",
    "\n",
    "money = keeper(dayclusters)\n",
    "events = money[['idx','day','ts','text','type','slot','cdex']]\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def dtconvert(x):\n",
    "    dt_object = datetime.fromtimestamp(x)\n",
    "    return dt_object\n",
    "\n",
    "# fix timestamp\n",
    "events['ts'] = events['ts'].apply(dtconvert)\n",
    "\n",
    "logging.info('Table of final headline Tweets generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump all to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file export - events\n",
    "with open('results_headlines.pkl', 'wb') as f:\n",
    "    pickle.dump(events, f)\n",
    "    \n",
    "# file export - wvoc_final\n",
    "with open('slot_termweights.pkl', 'wb') as f:\n",
    "    pickle.dump(wvoc_final, f)\n",
    "    \n",
    "# file export - clustervocs\n",
    "with open('cluster_vocabs.pkl', 'wb') as f:\n",
    "    pickle.dump(clustervocs, f)\n",
    "    \n",
    "f.close()\n",
    "\n",
    "logging.info('Files out')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
