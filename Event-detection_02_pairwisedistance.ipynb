{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# setup for logging\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# write logs with time to log folder\n",
    "LOG_FILENAME = datetime.now().strftime('~/log/logfile_%H_%M_%S_%d_%m_%Y.log')\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(filename=LOG_FILENAME,level=logging.DEBUG)\n",
    "\n",
    "# open file\n",
    "with open('df_preprocessed.pkl', 'rb') as f:\n",
    "    datastore = pickle.load(f)\n",
    "\n",
    "f.close()\n",
    "\n",
    "# make dataframe\n",
    "df_in = pd.DataFrame(datastore)\n",
    "\n",
    "# arrangements: 'slot' ommitted - thats the 2 hour windows in the json\n",
    "cols = ['idx', 'text', 'timestamp', 'ts', 'combined', 'ner_all', 'documents']\n",
    "df = df_in[cols]\n",
    "\n",
    "# replace values in timestamp with proper timestamp readable by Python\n",
    "df['timestamp'] = df['timestamp'].astype('datetime64[ns]')\n",
    "\n",
    "# sort by timestamp\n",
    "df = df.sort_values('ts', ascending=True)\n",
    "\n",
    "logging.info('Dataframe created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into 1 h time slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make slots \n",
    "\n",
    "tmin = 1369699200\n",
    "tmax = 1370908800\n",
    "bins = [i for i in range(tmin, tmax+3600, 3600)]\n",
    "labels = [i for i in range(len(bins)-1)]\n",
    "\n",
    "df['slot'] = pd.cut(df['ts'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# function that splits df by time frames\n",
    "def splitter(frame):\n",
    "    values = [i for i in range(0, df['slot'].max()+1)]\n",
    "    frames = []\n",
    "    for a in values:\n",
    "        df1 = frame.loc[frame['slot'] == a]\n",
    "        frames.append(df1)\n",
    "    return frames\n",
    "\n",
    "# apply\n",
    "frames = splitter(df)\n",
    "\n",
    "logging.info('Split complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sklearn\n",
    "import sklearn.feature_extraction\n",
    "\n",
    "# generates the arrays\n",
    "def vectorboy_arrays(lizt):\n",
    "    rays = []\n",
    "    for item in lizt:\n",
    "        num = (max(int(item.shape[0]*0.0025),8))\n",
    "        vectorizer = sklearn.feature_extraction.text.CountVectorizer(min_df=num)\n",
    "        corpus = item.documents.tolist()\n",
    "        X = vectorizer.fit_transform(corpus).toarray()\n",
    "        rays.append(X)\n",
    "    return rays\n",
    "\n",
    "# generates the corresponding vocabularies\n",
    "def vectorboy_vocabs(lizt):\n",
    "    vocabs = []\n",
    "    for item in lizt:\n",
    "        num = (max(int(item.shape[0]*0.0025),8))\n",
    "        vectorizer = sklearn.feature_extraction.text.CountVectorizer(min_df=num)\n",
    "        corpus = item.documents.tolist()\n",
    "        X = vectorizer.fit_transform(corpus).toarray()\n",
    "        vocabs.append(vectorizer.vocabulary_)\n",
    "    return vocabs\n",
    "\n",
    "# get vector matrix arrays list\n",
    "rays = vectorboy_arrays(frames)\n",
    "\n",
    "# get slot vocabs\n",
    "vocabs = vectorboy_vocabs(frames)\n",
    "\n",
    "logging.info('Vector matrices success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling & Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling and normalizing the vector matrices\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "def scaler(lizt):\n",
    "    output = []\n",
    "    for item in lizt:\n",
    "        scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "        X_scaled = scaler.fit_transform(item)\n",
    "        X_scaled = X_scaled.astype('float16')\n",
    "        output.append(X_scaled)\n",
    "    return output\n",
    "\n",
    "def normalizer(lizt):\n",
    "    output = []\n",
    "    for item in lizt:\n",
    "        X_norm = preprocessing.normalize(item)\n",
    "        X_norm = X_norm.astype('float16')\n",
    "        output.append(X_norm)\n",
    "    return output\n",
    "\n",
    "# scaling\n",
    "rays_scaled = scaler(rays)\n",
    "\n",
    "# normalizing\n",
    "rays_sn = normalizer(rays_scaled)\n",
    "\n",
    "logging.info('Normalizing success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute pairwise cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now pairwise similarity by cosine distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def cosine_sim(lizt):\n",
    "    output = []\n",
    "    for array in lizt:\n",
    "        array1 = array.astype('float16')\n",
    "        X_sim = 1-sklearn.metrics.pairwise.cosine_similarity(array1)\n",
    "        array2 = X_sim.astype('float16')\n",
    "        output.append(array2)\n",
    "    return output\n",
    "\n",
    "# partitioning data for memory efficiency\n",
    "rays1 = rays_sn[0:92]\n",
    "rays2 = rays_sn[92:200]\n",
    "rays3 = rays_sn[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARTITION 1\n",
    "\n",
    "import time\n",
    "\n",
    "# start timer\n",
    "t0 = time.time()\n",
    "\n",
    "# apply function\n",
    "result1 = cosine_sim(rays1)\n",
    "\n",
    "# record the time\n",
    "t1 = time.time()\n",
    "print(\"Time: {}\".format(t1-t0))\n",
    "\n",
    "logging.info('Partition 1 success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARTITION 2\n",
    "\n",
    "# start timer\n",
    "t0 = time.time()\n",
    "\n",
    "# apply function\n",
    "result2 = cosine_sim(rays2)\n",
    "\n",
    "# record the time\n",
    "t1 = time.time()\n",
    "print(\"Time: {}\".format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARTITION 3\n",
    "\n",
    "# start timer\n",
    "t0 = time.time()\n",
    "\n",
    "# apply function\n",
    "result3 = cosine_sim(rays3)\n",
    "\n",
    "# record the time\n",
    "t1 = time.time()\n",
    "print(\"Time: {}\".format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGGREGATE & File dump\n",
    "\n",
    "# aggregate\n",
    "rays_cs = result1 + result2 + result3 \n",
    "\n",
    "# saving arrays separately - very large files\n",
    "def saver(lizt):\n",
    "    for index, item in enumerate(lizt):\n",
    "        np.save('/home/wgrambozambo/arrays1/array'+ str(index)+'.npy', item)\n",
    "    print(\"Job complete, flies in /home/wgrambozambo/arrays1/\")\n",
    "\n",
    "# dump\n",
    "saver(rays_cs)\n",
    "\n",
    "logging.info('Arrays saved in arrays1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames, rays, and vocabs dump\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('frames.pkl', 'wb') as f:\n",
    "    pickle.dump(frames, f)\n",
    "    \n",
    "f.close()\n",
    "\n",
    "logging.info('List of dataframes by time slot saved as frames.pkl')\n",
    "\n",
    "with open('rays.pkl', 'wb') as f:\n",
    "    pickle.dump(rays, f)\n",
    "    \n",
    "f.close()\n",
    "\n",
    "logging.info('List of slot arrays saved as rays.pkl')\n",
    "\n",
    "with open('vocabs.pkl', 'wb') as f:\n",
    "    pickle.dump(vocabs, f)\n",
    "    \n",
    "f.close()\n",
    "\n",
    "logging.info('List of slot vocabs saved as vocabs.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
